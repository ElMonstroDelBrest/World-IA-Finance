# Strate IV — Latent Regime RL configuration (v6)
# obs_dim is auto-detected from buffer (3*d_model + n_tgt*4 + 5 + 4)
# v5.1: calmer le PPO — LR /3, batch x2, target_kl, plus de steps
# v6 (Phase E): TD-MPC2 + CVaR replaces PPO — use --mode tdmpc2 in train_strate_iv.py

env:
  n_tgt: 8              # episode length in patches (8 x 16h = 128h ~ 5.3 days)
  tc_rate: 0.002        # transaction cost per unit position change (20 bps)
  patch_len: 16          # candles per patch (must match Strate I/II tokenizer)
  dead_market_threshold: 1.0e-4  # relative vol below which signals are zeroed (noise gate)

buffer:
  buffer_dir: "data/trajectory_buffer_v5/"
  n_episodes: 5000       # v5: 10x more episodes (3000 multiverse + 2000 historical)
  n_futures: 16          # N future trajectories per episode
  val_ratio: 0.1         # fraction held out for evaluation

ppo:
  lr: 5.0e-5             # v5.1: /3 pour stabiliser (v5 avait approx_kl=0.70)
  n_steps: 512           # per env (x8 envs = 4096 effective steps per rollout)
  batch_size: 512        # v5.1: x2 pour lisser les gradients
  n_epochs: 4            # v5.1: /2 — moins de passes SGD par rollout = moins de KL drift
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.15       # v5.1: clip plus serré (0.2 -> 0.15)
  ent_coef: 0.01         # v5.1: remonté pour encourager l'exploration
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: 0.03        # v5.1: early stop epoch si KL > 0.03
  total_timesteps: 3000000  # v5.1: 3M pour laisser le temps de converger doucement
  eval_freq: 20000
  log_dir: "tb_logs/strate_iv_v51/"

tdmpc2:
  # === Architecture ===
  latent_dim: 256             # Planning latent dim (obs projected here for speed)
  hidden_dim: 512             # Hidden dim for all MLPs
  n_layers: 2                 # MLP depth (keep shallow for planning speed)
  n_quantiles: 32             # Quantiles for distributional critic (CVaR precision)
  # === Risk ===
  cvar_alpha: 0.25            # CVaR confidence: optimize worst-25% conditional expectation
  gamma: 0.99
  # === Optimizers ===
  lr: 3.0e-4
  ema_tau: 0.005              # Target critic EMA (slow: ~200 updates to sync)
  max_grad_norm: 10.0
  # === Replay ===
  batch_size: 256
  buffer_capacity: 100000
  warmup_steps: 1000          # Random exploration before first update
  update_freq: 1              # Update every env step
  # === MPPI planning ===
  use_planning: true
  plan_horizon: 5             # 5-step lookahead (5 patches × 16h ≈ 80h)
  plan_samples: 512           # K trajectory samples per MPPI call
  plan_iters: 6               # Refinement iterations
  plan_temperature: 0.5       # Softmax temperature (lower = more greedy)
  plan_init_std: 0.5          # Initial action std
  # === Training ===
  total_timesteps: 500000     # Online RL steps
  eval_freq: 5000
  log_dir: "tb_logs/strate_iv_v6/"
  save_dir: "checkpoints/strate_iv_tdmpc2/"
