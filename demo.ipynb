{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Financial-IA — Latent Market Intelligence Demo\n",
    "\n",
    "**End-to-end demo of the Strate IV PPO agent** operating in latent space learned by Fin-JEPA.\n",
    "\n",
    "This notebook:\n",
    "1. Installs dependencies and clones the repo\n",
    "2. Downloads pre-trained checkpoints (PPO agent + trajectory buffer)\n",
    "3. Runs the agent on held-out evaluation episodes\n",
    "4. Visualizes regime switching, position management, and PnL vs Buy & Hold\n",
    "\n",
    "**No training required** — inference only (~30 seconds on CPU, ~5s on GPU).\n",
    "\n",
    "---\n",
    "> Architecture: Spherical VQ-VAE → Fin-JEPA (Mamba-2) → Stochastic Predictor → PPO Agent  \n",
    "> Paper reference: LeCun (2022) *A Path Towards Autonomous Machine Intelligence* — JEPA framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch pytorch-lightning tslearn numpy pandas dacite pyyaml einops gymnasium stable-baselines3 matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone the repo (skip if already cloned)\n",
    "if not os.path.exists('World-IA-Finance'):\n",
    "    !git clone https://github.com/ElMonstroDelBrest/World-IA-Finance.git\n",
    "\n",
    "os.chdir('World-IA-Finance')\n",
    "print('Working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download-header",
   "metadata": {},
   "source": [
    "## 2. Download Pre-trained Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_URL = \"https://github.com/ElMonstroDelBrest/World-IA-Finance/releases/download/v1.0.0\"\n",
    "\n",
    "def download(url, dest):\n",
    "    dest = Path(dest)\n",
    "    if dest.exists():\n",
    "        print(f'  {dest} already exists, skipping.')\n",
    "        return\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(f'  Downloading {dest.name}...')\n",
    "    urllib.request.urlretrieve(url, dest)\n",
    "    print(f'  Done ({dest.stat().st_size / 1e6:.1f} MB)')\n",
    "\n",
    "# PPO agent checkpoint\n",
    "print('Downloading PPO agent...')\n",
    "download(\n",
    "    f\"{BASE_URL}/ppo_strate_iv_1000000_steps.zip\",\n",
    "    \"checkpoints/strate_iv/ppo_strate_iv_1000000_steps.zip\"\n",
    ")\n",
    "\n",
    "# Pre-computed trajectory buffer (JEPA latent representations)\n",
    "print('Downloading trajectory buffer...')\n",
    "download(\n",
    "    f\"{BASE_URL}/trajectory_buffer.zip\",\n",
    "    \"/tmp/trajectory_buffer.zip\"\n",
    ")\n",
    "\n",
    "# Extract trajectory buffer\n",
    "buf_dir = Path('data/trajectory_buffer')\n",
    "if not buf_dir.exists() or not any(buf_dir.glob('*.pt')):\n",
    "    print('Extracting trajectory buffer...')\n",
    "    with zipfile.ZipFile('/tmp/trajectory_buffer.zip', 'r') as z:\n",
    "        z.extractall('.')\n",
    "    print(f'  Extracted {len(list(buf_dir.glob(\"*.pt\")))} episodes')\n",
    "else:\n",
    "    print(f'  Buffer already extracted: {len(list(buf_dir.glob(\"*.pt\")))} episodes')\n",
    "\n",
    "print('\\nAll assets ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-header",
   "metadata": {},
   "source": [
    "## 3. Run Agent Demo\n",
    "\n",
    "The PPO agent operates on **latent observations** — not raw prices. Each step, it receives:\n",
    "- The JEPA context encoding of past market regimes\n",
    "- A distribution of N=16 stochastic future trajectories\n",
    "- Its current position and cumulative PnL\n",
    "\n",
    "It outputs a continuous action in `[-1, 1]` (short → flat → long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-demo",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '.')\n\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, Image\nfrom pathlib import Path\nimport gymnasium as gym\n\nfrom stable_baselines3 import PPO\nfrom src.strate_iv.config import load_config\nfrom src.strate_iv.env import LatentCryptoEnv\nfrom src.strate_iv.trajectory_buffer import TrajectoryBuffer\n\n# Load config and buffer\nconfig = load_config('configs/strate_iv.yaml')\nbuffer = TrajectoryBuffer('data/trajectory_buffer/')\n_, eval_buffer = buffer.split(val_ratio=config.buffer.val_ratio)\nprint(f'Eval buffer: {len(eval_buffer)} episodes')\n\n# Load PPO agent — detect expected obs dim from saved model\nmodel_path = 'checkpoints/strate_iv/ppo_strate_iv_1000000_steps.zip'\nmodel = PPO.load(model_path)\nexpected_obs_dim = model.observation_space.shape[0]\nprint(f'PPO agent loaded — expected obs dim: {expected_obs_dim}')\n\n# Create environment\nenv = LatentCryptoEnv(buffer=eval_buffer, config=config.env)\nactual_obs_dim = env.observation_space.shape[0]\nprint(f'Environment obs dim: {actual_obs_dim}')\n\n# Compatibility shim: env was updated after training, truncate obs to match\nif actual_obs_dim != expected_obs_dim:\n    print(f'Obs dim mismatch ({actual_obs_dim} vs {expected_obs_dim}) — applying compatibility shim')\n    class ObsCompatWrapper(gym.ObservationWrapper):\n        def __init__(self, env, target_dim):\n            super().__init__(env)\n            self.target_dim = target_dim\n            self.observation_space = gym.spaces.Box(\n                low=-np.inf, high=np.inf, shape=(target_dim,), dtype=np.float32\n            )\n        def observation(self, obs):\n            if len(obs) >= self.target_dim:\n                return obs[:self.target_dim]\n            return np.pad(obs, (0, self.target_dim - len(obs)))\n    env = ObsCompatWrapper(env, expected_obs_dim)\n\nprint(f'Environment: {config.env.n_tgt} patches x {config.env.patch_len} candles = {config.env.n_tgt * config.env.patch_len}h window')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-episodes",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(model, env, seed=None):\n",
    "    \"\"\"Run one episode and collect trajectory data.\"\"\"\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    realized_idx = info['realized_future_idx']\n",
    "    entry = env._entry\n",
    "    realized = entry.future_ohlcv[realized_idx].numpy()  # (N_tgt, patch_len, 5)\n",
    "\n",
    "    actions, positions, cum_pnls = [], [], []\n",
    "\n",
    "    for _ in range(config.env.n_tgt):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, _, terminated, _, step_info = env.step(action)\n",
    "        actions.append(float(action[0]))\n",
    "        positions.append(step_info['position'])\n",
    "        cum_pnls.append(step_info['cumulative_pnl'])\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        'realized_ohlcv': realized,\n",
    "        'actions': np.array(actions),\n",
    "        'positions': np.array(positions),\n",
    "        'cum_pnls': np.array(cum_pnls),\n",
    "        'last_close': entry.last_close,\n",
    "        'sigma_close': entry.revin_stds[0, 3].item(),\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_episode(traj, output_path, title_suffix=''):\n",
    "    \"\"\"3-panel visualization: price + position + PnL vs B&H.\"\"\"\n",
    "    realized = traj['realized_ohlcv']\n",
    "    n_tgt, patch_len, _ = realized.shape\n",
    "\n",
    "    close_flat = realized[:, :, 3].flatten()\n",
    "    time_x = np.arange(len(close_flat))\n",
    "    patch_mids = np.arange(n_tgt) * patch_len + patch_len // 2\n",
    "    patch_edges = np.arange(n_tgt + 1) * patch_len\n",
    "\n",
    "    bh_returns = (close_flat - close_flat[0]) / (close_flat[0] + 1e-8)\n",
    "\n",
    "    # Reconstruct agent PnL in price space\n",
    "    agent_pnl = np.zeros(len(close_flat))\n",
    "    cum = 0.0\n",
    "    for t in range(n_tgt):\n",
    "        a = traj['actions'][t]\n",
    "        for c in range(patch_len):\n",
    "            idx = t * patch_len + c\n",
    "            if idx > 0:\n",
    "                ret = (close_flat[idx] - close_flat[idx-1]) / (close_flat[idx-1] + 1e-8)\n",
    "                cum += a * ret\n",
    "            agent_pnl[idx] = cum\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True,\n",
    "                              gridspec_kw={'height_ratios': [3, 1.5, 2]})\n",
    "    fig.patch.set_facecolor('#0d1117')\n",
    "    for ax in axes:\n",
    "        ax.set_facecolor('#161b22')\n",
    "        ax.tick_params(colors='#8b949e')\n",
    "        ax.spines[:].set_color('#30363d')\n",
    "        ax.yaxis.label.set_color('#8b949e')\n",
    "        ax.xaxis.label.set_color('#8b949e')\n",
    "\n",
    "    # Panel 1: Price\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(time_x, close_flat, color='#58a6ff', linewidth=1.2, label='Close')\n",
    "    for t in range(n_tgt):\n",
    "        a = traj['actions'][t]\n",
    "        color = '#238636' if a > 0.1 else '#da3633' if a < -0.1 else '#6e7681'\n",
    "        alpha = min(0.35, abs(a) * 0.35) if abs(a) > 0.1 else 0.05\n",
    "        ax1.axvspan(patch_edges[t], patch_edges[t+1], color=color, alpha=alpha)\n",
    "    ax1.set_ylabel('Close Price')\n",
    "    ax1.set_title(\n",
    "        f'Strate IV — Latent Regime RL Agent{title_suffix}   |   '\n",
    "        f'σ_close = {traj[\"sigma_close\"]:.4f}   |   '\n",
    "        f'Last context close = {traj[\"last_close\"]:.4f}',\n",
    "        color='#e6edf3', fontsize=11\n",
    "    )\n",
    "    ax1.legend(facecolor='#161b22', labelcolor='#e6edf3')\n",
    "    ax1.grid(True, alpha=0.2, color='#30363d')\n",
    "\n",
    "    # Panel 2: Position\n",
    "    ax2 = axes[1]\n",
    "    colors = ['#238636' if a > 0 else '#da3633' if a < 0 else '#6e7681'\n",
    "              for a in traj['actions']]\n",
    "    ax2.bar(patch_mids, traj['actions'], width=patch_len * 0.8,\n",
    "            color=colors, alpha=0.85, edgecolor='#30363d', linewidth=0.5)\n",
    "    ax2.axhline(0, color='#8b949e', linewidth=0.8)\n",
    "    ax2.set_ylabel('Position')\n",
    "    ax2.set_ylim(-1.2, 1.2)\n",
    "    ax2.set_yticks([-1, -0.5, 0, 0.5, 1])\n",
    "    ax2.grid(True, alpha=0.2, color='#30363d')\n",
    "\n",
    "    # Panel 3: PnL\n",
    "    ax3 = axes[2]\n",
    "    ax3.plot(time_x, agent_pnl * 100, color='#3fb950',\n",
    "             linewidth=2.2, label='Agent (Latent RL)')\n",
    "    ax3.plot(time_x, bh_returns * 100, color='#8b949e',\n",
    "             linewidth=1.5, linestyle='--', label='Buy & Hold')\n",
    "    ax3.axhline(0, color='#6e7681', linewidth=0.6)\n",
    "    ax3.set_ylabel('Cumulative Return (%)')\n",
    "    ax3.set_xlabel(f'Candles (1h)  |  {n_tgt} patches × {patch_len} candles = {n_tgt * patch_len}h window')\n",
    "    ax3.legend(facecolor='#161b22', labelcolor='#e6edf3')\n",
    "    ax3.grid(True, alpha=0.2, color='#30363d')\n",
    "\n",
    "    agent_final = agent_pnl[-1] * 100\n",
    "    bh_final = bh_returns[-1] * 100\n",
    "    alpha_val = agent_final - bh_final\n",
    "    color_box = '#1a4731' if alpha_val >= 0 else '#4a1515'\n",
    "    ax3.annotate(\n",
    "        f'Agent:  {agent_final:+.2f}%\\nB&H:    {bh_final:+.2f}%\\nAlpha: {alpha_val:+.2f}%',\n",
    "        xy=(0.98, 0.95), xycoords='axes fraction', ha='right', va='top',\n",
    "        fontsize=10, fontfamily='monospace', color='#e6edf3',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor=color_box, edgecolor='#30363d')\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight', facecolor=fig.get_facecolor())\n",
    "    plt.close(fig)\n",
    "    return agent_final, bh_final, alpha_val\n",
    "\n",
    "\n",
    "# Run 5 episodes\n",
    "Path('outputs/demo').mkdir(parents=True, exist_ok=True)\n",
    "print('Running 5 evaluation episodes...\\n')\n",
    "results = []\n",
    "for i in range(5):\n",
    "    traj = run_episode(model, env, seed=i)\n",
    "    out = f'outputs/demo/episode_{i:02d}.png'\n",
    "    agent_r, bh_r, alpha = plot_episode(traj, out, title_suffix=f'  —  Episode {i+1}/5')\n",
    "    results.append((agent_r, bh_r, alpha))\n",
    "    sign = '+' if alpha >= 0 else ''\n",
    "    print(f'  Episode {i+1}: Agent {agent_r:+.2f}% | B&H {bh_r:+.2f}% | Alpha {sign}{alpha:.2f}%')\n",
    "\n",
    "print(f'\\nMean alpha over 5 episodes: {np.mean([r[2] for r in results]):+.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "for i in range(5):\n",
    "    agent_r, bh_r, alpha = results[i]\n",
    "    print(f'\\n--- Episode {i+1} | Agent: {agent_r:+.2f}% | B&H: {bh_r:+.2f}% | Alpha: {alpha:+.2f}% ---')\n",
    "    display(Image(filename=f'outputs/demo/episode_{i:02d}.png', width=900))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "### What you just saw\n",
    "\n",
    "The agent **never sees raw prices** during inference. It operates entirely on latent representations produced by Fin-JEPA:\n",
    "\n",
    "| Component | Role |\n",
    "|---|---|\n",
    "| **Spherical VQ-VAE** (Strate I) | Tokenizes OHLCV patches → discrete market regime tokens |\n",
    "| **Fin-JEPA + Mamba-2** (Strate II) | Self-supervised temporal model over token sequences |\n",
    "| **Stochastic Predictor** (Strate III) | Samples N=16 divergent future latent trajectories |\n",
    "| **PPO Agent** (Strate IV) | Plans in latent space, outputs continuous position [-1, 1] |\n",
    "\n",
    "### Why this matters\n",
    "\n",
    "Classical approaches (LSTM, Transformer on raw prices) are forced to predict **every tick** — memorizing noise instead of learning structure.  \n",
    "JEPA learns to predict **latent representations** of future states, ignoring unpredictable details.  \n",
    "The agent then plans in this cleaner latent space, exhibiting genuine **regime switching** rather than curve-fitting.\n",
    "\n",
    "---\n",
    "\n",
    "**Repository:** https://github.com/ElMonstroDelBrest/World-IA-Finance  \n",
    "**License:** AGPL-3.0"
   ]
  }
 ]
}